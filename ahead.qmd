

```{python echo=F}
def linear_model_simple(X,cat,y=None):
    '''
    Define linear model with priors for the parameters and model error
    Inputs:
        energy: storm energy
        dshl: observed shoreline change
    '''
    # Define priors
    with numpyro.plate("cats", n_cat):
        intercept = numpyro.sample("intercept",dist.Normal(0, 10)) 
        b_Eocum = numpyro.sample("b_Eocum",dist.Normal(0, 10)) 
        b_WLres = numpyro.sample("b_WLres",dist.Normal(0, 10))
        b_Tpeak = numpyro.sample("b_Tpeak",dist.Normal(0, 10))
        b_sinDrel = numpyro.sample("b_sinDrel",dist.Normal(0, 10))
        b_Wpre = numpyro.sample("b_Wpre",dist.Normal(0, 10)) 
    # jax.debug.print("{}",b_WLres)
        sigma = numpyro.sample("sigma",dist.Exponential(1))

    # jax.debug.print("{}",intercept[cat])

    mu = b_Eocum[cat] * X[:,0] + b_WLres[cat] * X[:,1] + b_Tpeak[cat] * X[:,2] + b_sinDrel[cat] * X[:,3] + b_Wpre[cat] * X[:,4] + intercept[cat]

    # store the model prediction before we account for the error
    numpyro.deterministic("mu", mu)
    # and then finally sample so we can compare to our observations
    numpyro.sample("y_out", dist.Normal(mu, sigma[cat]), obs=y)
```

```{python echo=F}
def linear_model(X,cat,y=None):
    '''
    Define linear model with priors for the parameters and model error
    Inputs:
        energy: storm energy
        dshl: observed shoreline change
    '''

    intercept_bar = numpyro.sample("intercept_bar",dist.Normal(0, 10)) 
    b_Eocum_bar = numpyro.sample("b_Eocum_bar",dist.Normal(0, 10)) 
    b_WLres_bar = numpyro.sample("b_WLres_bar",dist.Normal(0, 10))
    b_Tpeak_bar = numpyro.sample("b_Tpeak_bar",dist.Normal(0, 10))
    b_sinDrel_bar = numpyro.sample("b_sinDrel_bar",dist.Normal(0, 10))
    b_Wpre_bar = numpyro.sample("b_Wpre_bar",dist.Normal(0, 10)) 

    # Define priors
    tau = 0.1
    with numpyro.plate("cats", n_cat):
        intercept = numpyro.sample("intercept",dist.Normal(intercept_bar, tau)) 
        b_Eocum = numpyro.sample("b_Eocum",dist.Normal(b_Eocum_bar, tau)) 
        b_WLres = numpyro.sample("b_WLres",dist.Normal(b_WLres_bar, tau))
        b_Tpeak = numpyro.sample("b_Tpeak",dist.Normal(b_Tpeak_bar, tau))
        b_sinDrel = numpyro.sample("b_sinDrel",dist.Normal(b_sinDrel_bar, tau))
        b_Wpre = numpyro.sample("b_Wpre",dist.Normal(b_Wpre_bar, tau)) 

    # jax.debug.print("{}",b_WLres)
        sigma = numpyro.sample("sigma",dist.Exponential(1))

    # jax.debug.print("{}",intercept[cat])

    mu = b_Eocum[cat] * X[:,0] + b_WLres[cat] * X[:,1] + b_Tpeak[cat] * X[:,2] + b_sinDrel[cat] * X[:,3] + b_Wpre[cat] * X[:,4] + intercept[cat]

    # store the model prediction before we account for the error
    numpyro.deterministic("mu", mu)
    # and then finally sample so we can compare to our observations
    numpyro.sample("y_out", dist.Normal(mu, sigma[cat]), obs=y)
```

```{python echo=F}
avg_dir = 135 #in degrees
df =  df_storms.copy()
df['sinDrel'] = np.sin((df['Dpo']-avg_dir)*np.pi/180)
df

X = jnp.array(df[id_vars+['Wpre']].values)
cat = jnp.array(df['variable'].astype('category').cat.codes).astype(jnp.int32)
y = jnp.array(df['value'].values)

X = (X - X.mean(axis=0))/X.mean(axis=0)

y_mean = y.mean() 
y_scale = y.std() 
y = (y-y_mean)/y_scale

n_cat = df['variable'].astype('category').cat.codes.max()+1

```


```{python echo=F}
# JAX requires a key for random number generation
rng_key_ = random.PRNGKey(2101)
# here take 100 samples from our priors and make predictions on x_log
prior_samples = Predictive(linear_model, num_samples=500)(
    rng_key_, X=X, cat=cat
)
prior_samples = {k: jnp.expand_dims(v,axis=0) for k, v in prior_samples.items()}
# and put this into arviz for easy plotting
arviz_priors = az.from_dict(
    prior=prior_samples
)
arviz_priors

```


```{python echo=F}
var_names = ['b_{}'.format(_) for _ in id_vars]

```


```{python echo=F}
# and now plot the distributions and the simulated data
print('#'*80)
print('Priors')
priors_ax = az.plot_trace(
    arviz_priors.prior, 
    var_names=var_names,
    figsize=(8,6),
    rug=True,
    show=False,
    combined=True
)
# make it readable
plt.subplots_adjust(hspace=0.5)
plt.show()

```

# Now we sample

```{python echo=F}

# settings 
num_samples = 1000
num_warmup = num_samples
num_chains = 3
ci = 0.89

# JAX requires a key for random number generation
rng_key_ = random.PRNGKey(2101)

# define the sampler - No U-Turn Sampler (NUTS)
kernel = NUTS(linear_model)

# define the mcmc wrapper
mcmc_obj = MCMC(
    kernel, 
    num_warmup=num_warmup, 
    num_samples=num_samples,
    num_chains=num_chains
)

# now run the sampler for num_samples+burnin
mcmc_obj.run(
    rng_key_, X=X, cat=cat, y=y
)
mcmc_obj.print_summary()
# get the samples which will form our posterior
samples = mcmc_obj.get_samples()

# get the samples for predictive uncertainty (our linear model + error)
posterior_predictive = Predictive(linear_model, samples)(
    rng_key_, X=X, cat=cat, y=y
)

# get the mean model prediciton
mean_mu = jnp.mean(posterior_predictive['mu'], axis=0)
# hpdi is used to compute the credible intervals corresponding to ci
hpdi_mu = hpdi(posterior_predictive['mu'], ci)
hpdi_sim_y = hpdi(posterior_predictive['y_out'], ci)

arviz_posterior = az.from_numpyro(
    mcmc_obj,
    prior=prior_samples,
    posterior_predictive=posterior_predictive,
)
# arviz_posterior
```


```{python echo=F}
# JAX requires a key for random number generation
rng_key_ = random.PRNGKey(2101)

# define the sampler - No U-Turn Sampler (NUTS)
kernel = NUTS(linear_model_simple)

# define the mcmc wrapper
mcmc_obj_simple = MCMC(
    kernel, 
    num_warmup=num_warmup, 
    num_samples=num_samples,
    num_chains=num_chains
)

# now run the sampler for num_samples+burnin
mcmc_obj_simple.run(
    rng_key_, X=X, cat=cat, y=y
)
mcmc_obj_simple.print_summary()
# get the samples which will form our posterior
samples_simple = mcmc_obj.get_samples()

# get the samples for predictive uncertainty (our linear model + error)
posterior_predictive_simple = Predictive(linear_model_simple, samples_simple)(
    rng_key_, X=X, cat=cat, y=y
)

# get the mean model prediciton
mean_mu_simple = jnp.mean(posterior_predictive_simple['mu'], axis=0)
# hpdi is used to compute the credible intervals corresponding to ci
hpdi_mu_simple = hpdi(posterior_predictive_simple['mu'], ci)
hpdi_sim_y_simple = hpdi(posterior_predictive_simple['y_out'], ci)

arviz_posterior_simple = az.from_numpyro(
    mcmc_obj_simple,
    posterior_predictive=posterior_predictive_simple,
)
# arviz_posterior

```


```{python echo=F}

print('#'*80)
print('Posterior')
az.plot_trace(
    arviz_posterior,
    var_names=var_names + ['intercept'],
    figsize=(8,6),
    show=False
)
# make it readable
plt.subplots_adjust(hspace=0.5)
plt.show()
```


```{python echo=F}
print('#'*80)
print('Posterior')
az.plot_forest(
    [arviz_posterior, arviz_posterior_simple],
    var_names=var_names+ ['intercept'],
    figsize=(4,10),
    combined=True
)

```



```{python echo=F}
results_df = df[['variable','value']].copy()
results_df['mod_y'] = mean_mu * y_scale + y_mean
results_df
results_df_simple  = df[['variable','value']].copy()
results_df_simple ['mod_y'] = mean_mu_simple * y_scale + y_mean
results_df_simple 

```



```{python echo=F}

fig,axs = plt.subplots(1,3,figsize = (15,5))
axs =  axs.ravel()
RMSE = [] 
R2   = []
colors = ['tab:blue','tab:orange','tab:green']
storms = []
for i,case in enumerate(cases):
    case_results_df = results_df[results_df['variable']==case]
    score = r2_score(case_results_df['value'].values,case_results_df['mod_y'].values) #Coef of determination
    rmse = np.sqrt(mean_squared_error(case_results_df['value'].values,case_results_df['mod_y'].values)) #root mean squared error

    #Plot
    textstr = '\n'.join((r'$R^{2}=%.2f$' % (score, ),
                            r'RMSE=$%.1f$ m' % (rmse, )))
    props = dict(boxstyle='round', facecolor='lightgrey')
    axs[i].text(0.03, 0.14, textstr, transform=axs[i].transAxes, fontsize=10,
            verticalalignment='top', bbox=props)
    axs[i].plot(case_results_df['value'].values,case_results_df['mod_y'].values,'.', label = None, color = colors[i],markersize=8)
    axs[i].plot(np.arange(-30,45,0.5),np.arange(-30,45,0.5),linestyle = 'dashed', color = 'black',label = '1:1')
    axs[i].set_xlim([-20,45])
    axs[i].set_ylim([-15,30])
    axs[i].axvline(x=0, color = 'lightgrey', linestyle = 'dashed')
    axs[i].axhline(y=0, color = 'lightgrey', linestyle = 'dashed')
    axs[i].set_xlabel(r'$\Delta W$'+'(m) - observed')
    axs[i].set_ylabel(r'$\Delta W$'+'(m) - multi-linear regression')
    axs[i].legend(loc = 'lower right')
    axs[i].set_title(CASES[i])  

    fig.suptitle(r'$\Delta W$' + ' - Descriptive approach')
    for ax in axs:
        ax.grid(linestyle = 'dashed', color = 'lightgrey')
    plt.tight_layout()
plt.show()
```



```{python echo=F}
fig,axs = plt.subplots(1,3,figsize = (15,5))
axs =  axs.ravel()
RMSE = [] 
R2   = []
colors = ['tab:blue','tab:orange','tab:green']
storms = []
for i,case in enumerate(cases):
    case_results_df = results_df_simple[results_df_simple['variable']==case]
    score = r2_score(case_results_df['value'].values,case_results_df['mod_y'].values) #Coef of determination
    rmse = np.sqrt(mean_squared_error(case_results_df['value'].values,case_results_df['mod_y'].values)) #root mean squared error

    #Plot
    textstr = '\n'.join((r'$R^{2}=%.2f$' % (score, ),
                            r'RMSE=$%.1f$ m' % (rmse, )))
    props = dict(boxstyle='round', facecolor='lightgrey')
    axs[i].text(0.03, 0.14, textstr, transform=axs[i].transAxes, fontsize=10,
            verticalalignment='top', bbox=props)
    axs[i].plot(case_results_df['value'].values,case_results_df['mod_y'].values,'.', label = None, color = colors[i],markersize=8)
    axs[i].plot(np.arange(-30,45,0.5),np.arange(-30,45,0.5),linestyle = 'dashed', color = 'black',label = '1:1')
    axs[i].set_xlim([-20,45])
    axs[i].set_ylim([-15,30])
    axs[i].axvline(x=0, color = 'lightgrey', linestyle = 'dashed')
    axs[i].axhline(y=0, color = 'lightgrey', linestyle = 'dashed')
    axs[i].set_xlabel(r'$\Delta W$'+'(m) - observed')
    axs[i].set_ylabel(r'$\Delta W$'+'(m) - multi-linear regression')
    axs[i].legend(loc = 'lower right')
    axs[i].set_title(CASES[i])  

    fig.suptitle(r'$\Delta W$' + ' - Descriptive approach')
    for ax in axs:
        ax.grid(linestyle = 'dashed', color = 'lightgrey')
    plt.tight_layout()
plt.show()

```


```{python echo=F}

fig,axs = plt.subplots(1,3,figsize = (15,5))
axs =  axs.ravel()
RMSE = [] 
R2   = []
colors = ['tab:blue','tab:orange','tab:green']
storms = []

for i,case in enumerate(cases):
    
    casewi = 'Wpre'+case[2:]
    df =  df_storms.copy()
    df['sinDrel'] = np.sin((df['Dpo']-avg_dir)*np.pi/180)
    
    df =  df[['Eocum','WLres','Tpeak','sinDrel',casewi, case]]
    
    #renaming
    df.columns = ['CumEOff', 'WLres','Tpeak','sinDrel','Wpre', 'dW']
    
    df =  df.dropna()   

    #Linear regression with intercept
    LR = LinearRegression(fit_intercept=True) 
    LR.fit(df[['CumEOff', 'WLres', 'Tpeak', 'sinDrel', 'Wpre']],df.dW)
    
    
    y_prediction =  LR.predict(df[['CumEOff', 'WLres', 'Tpeak', 'sinDrel', 'Wpre']])
    
    score=r2_score(df.dW,y_prediction) #Coef of determination
    rmse = np.sqrt(mean_squared_error(df.dW,y_prediction)) #root mean squared error
    
    #Plot
    
    textstr = '\n'.join((r'$R^{2}=%.2f$' % (score, ),
                         r'RMSE=$%.1f$ m' % (rmse, )))
    props = dict(boxstyle='round', facecolor='lightgrey')
    axs[i].text(0.03, 0.14, textstr, transform=axs[i].transAxes, fontsize=10,
            verticalalignment='top', bbox=props)
    axs[i].plot(df['dW'],y_prediction,'.', label = None, color = colors[i],markersize=8)
    axs[i].plot(np.arange(-30,45,0.5),np.arange(-30,45,0.5),linestyle = 'dashed', color = 'black',label = '1:1')
    axs[i].set_xlim([-20,45])
    axs[i].set_ylim([-15,30])
    axs[i].axvline(x=0, color = 'lightgrey', linestyle = 'dashed')
    axs[i].axhline(y=0, color = 'lightgrey', linestyle = 'dashed')
    axs[i].set_xlabel(r'$\Delta W$'+'(m) - observed')
    axs[i].set_ylabel(r'$\Delta W$'+'(m) - multi-linear regression')
    axs[i].legend(loc = 'lower right')
    axs[i].set_title(CASES[i])  

fig.suptitle(r'$\Delta W$' + ' - Descriptive approach')
for ax in axs:
    ax.grid(linestyle = 'dashed', color = 'lightgrey')
plt.tight_layout()
```

Too often we can get trapped thinking about a fixed model structure being repeatedly applied at different locations. I would argue that where possible we should think about data more whollistically, we are modelling a very similar process at three separate locations. So why not fit all the data within a single model, *categorised* by location?

This opens up a very cool (:fire:?) possibility: sharing information about model parameters between locations. Sharing is caring after all and our models, doing their best to interpret the messy environmental data we give them, often need a little bit of TLC.

I'll leave it there with this absolute cliff hanger, tune in next time for more.
---
title: "Bayesian linear regression example"
theme: flatly
from: markdown+emoji
format:
  html:
    toc: true
    number-sections: true
editor: source
execute:
  cache: true
---

# An ode to linear regressions

Good old fashioned, handful of parameters linear regressions are great!

Don"t get me wrong I love fitting larger machine learning (ML) models and when you"re after raw predictive power there"s nothing better than finally getting that  huge neural networking singing. But in so many applications where you would employ data-driven models, linear regression models (and variations thereof) can hold their own and often be better suited to the project as a whole. 

There"s something very tangible about knowing all the parameters in your model. A tight knit group that you can all call friends when you push that final project deliverable out the door. As ML models get larger, at best they"re are filled with acquaintances. More often you"re left anxiously gazing into the black-box abyss just hoping to recognise a friendly face.

For a while now my default method for fitting linear regressions has been Bayesian. Sure uncertainty quantification is :fire:, but really it"s the flexibility afforded by a Bayesian framework which lets you guide and extend on simple models to provide just the right level of complexity. Actually this post is really a vehicle to a future post which will dive into some of this flexibility and really show off the interesting aspects (hint: it"ll be plenty hierarchical). But we have to get on the bus somewhere, and this is the stop closest to home. 

I will say though, that you pay for all these flashy uncertainty bands et al. with a (mostly very modest) computational cost. Sometimes it can sneak up on you for more complex models, but for a lot of problems in the environmental space the time difference is small on modern compute. I"ve skimmed through enough articles to know attention spans are shorter than ever, or something to that effect, but surely we can spare a extra seconds to give our model a glow up.

## What this is and what this is not

::: {.callout-tip}
## IS
A very practical implementation based introduction to Bayesian methods.
:::

::: {.callout-important}
## IS NOT
A comprehensive guide. I"ll try refer to other books where more details are needed.
:::

## The task

We are going to use a great, openly available dataset that describes sandy beach response to offshore storms (as measured by shoreline change). This comes from the excellent paper:

Data-driven modelling of coastal storm erosion for real-time forecasting at a wave-dominated embayed beach (Ibaceta and Harley, 2024)
https://doi.org/10.1016/j.coastaleng.2024.104596

NB: This is not in any way meant to be knock on the use of a frequentist approach in the above paper. I"ll show some of the differences as we go along of course, but the true power of applying Bayesian methods only comes when fitting more complex models which we will see in the next post. I use this paper as the data are available, the model has been prepared and carefully thought about, and it was an excellent (and clear/well-written) use of data-driven modelling to explore storm erosion!

Anyway I hear you, pipe down and show me some data.

# Beginning

## The data

```{python}
#| include: false

import os
# cpu cores for sampling
os.environ["XLA_FLAGS"] = "--xla_force_host_platform_device_count=4"

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, root_mean_squared_error

import matplotlib.pyplot as plt
import seaborn as sns


# NumPyro for proabilistic programming
import arviz as az
from jax import random
import jax
import jax.numpy as jnp
import numpyro
from numpyro.diagnostics import hpdi
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS
from numpyro.infer import Predictive
```

We load the data directly from the authors" github repo (thanks again!). I"m going to walk you through a bit of the data wrangling. I often find it illuminating to see the ways people transform data to get it "analysis ready", but is you wish to skip to the juicier bits below, be my guest.

As per the selected model in the paper, we will model the change in shoreline position at a given location (`dW`) with the following variables:

- `Wpre`: The shoreline position before the storm
- `Eocum`: cumulative offshore wave energy during the storm (proportional to significant wave height squared)
- `WLres`: residual of measured water level to astronomical tide
- `Tpeak`: peak offshore wave period during the storm
- `Dpo`: average wave direction during the storm

```{python}
#| echo: false
 
# Load data from the github repository for the aforementioned paper
fn = "https://raw.githubusercontent.com/raiibacetav/data-driven-storm-erosion/refs/heads/main/data.csv"
raw_data = pd.read_csv(fn, index_col=0)
# lets take a peek
display(raw_data.iloc[90:95])
```

```{python}
#| output: false

# Define the variables we are interested in
cases = [ "dW_exposed","dW_partially", "dW_sheltered"]
id_vars = ["Onset","Eocum","WLres","Tpeak","Dpo"] # our covariates and Onset for sanity
pre_vars = ["Wpre{}".format(_[2:]) for _ in cases]
```

We are going to make a little format adjustment just to make filtering easier, there are three separate locations along which shoreline change was measured at Narrabeen (named here as exposed, partially and sheltered). These are provided as columns and we will rework the dataframe to have only a single target variable (`dW`) and convert locations into a category.

```{python}
#| output: false
#| 
# rework the dataframe
# take only the columns we need
df =  raw_data.copy()[id_vars+pre_vars+cases]
# convert Onset to datetime
df["Onset"] = pd.to_datetime(df["Onset"], dayfirst=True)
# we will convert 3 columns into 2 (category and value) by melting, melting - oh what a world
df = pd.melt(df, id_vars = id_vars + pre_vars, var_name="location", value_name="dW").reset_index(drop=True)
# we"re going similarly grab the correct prestorm beach position based on the category, the lazy way
for ii in np.arange(df.shape[0]):
    df.loc[ii,"Wpre"] = df.loc[ii,"Wpre{}".format(df.loc[ii,"location"][2:])] 
# drop the now obsolete Wpre columns
df = df.drop(columns=pre_vars)
# lets just remove all the unnecessary dW_ in the variable column
df["location"] = df["location"].str.replace("dW_","")
df.sort_values(by=["Onset","location"], inplace=True)
# and lets get rid of any NaNs
df =  df.dropna().reset_index(drop=True)
```

After a bit of ugly wrangling we have our data in the format we want.

```{python}
#| echo: false
 
display(df.head())
```

You should be able to see the 1999-02-05 storm correctly flowing from `raw_data` through to `df`.

With this shift to categories, I am also signalling our intent down the track. More on this at the end of the post. In the sections below we will focus on the basics of implementing a Bayesian regression.

## The model

We follow the model provided in the paper:

$$\Delta W = \beta_0 + \beta_1 E_{o,cum} + \beta_2 W_{pre} + \beta_3 D_{po} + \beta_4 T_{p,peak} + \beta_5 WL_{res} + \epsilon$$

We have a model that predicts the change in shoreline ($\Delta W$) from the additive combination of our five variables (along with an intecept term $\beta_0$). 

We"re limited, of course, by the amount of words that I can responsibly subject you to. So for anyone wanting a comprehensive resource that introduces linear regression from the very basics (and goes far further than this blog too!) - [Regression and Other Stories by Gelman, Hill and Vehtari](https://users.aalto.fi/~ave/ROS.pdf).

But onto the little red caboose languishing at the end, $\epsilon$. Sometimes it doesn"t get the attention of the cars up front, but it"s a workhorse in any gradient ascent. To help us intuit the Bayesian approach to fitting, lets simplify our model. We have our five variables, plus an intercept, which we will label collectively as $\mathbf{X}$ and coefficients for each that we must fit which we label collectively as $\mathbf{\beta}$ (see [the helpful wiki on linear regression if this notation is unfamiliar](https://en.wikipedia.org/wiki/Linear_regression#Formulation)). 

We can condense our model to:

$$
\Delta W = \mathbf{X} \mathbf{\beta} + \epsilon
$$

But what is $\epsilon$ in a Bayesian regression? One way to describe it would be that:

- $\mathbf{X} \mathbf{\beta}$ describes our model estimating the values of $\Delta W$
- $\epsilon$ describes the residuals or errors of our model when considering our data

In this case we are going to assume that our residuals are normally distributed with a standard deviation of $\sigma$ and centered around zero ($\mathcal{N}(0, \sigma)$). Why? This is a good start and a common assumption. Lot of stuff in nature is normal, it"s hip to be normal. Plus thinking about the alternatives too hard would lead us to the scary world of generalised linear models, and that"s not for today my friend.

Okay so:

$$
\Delta W = \mathbf{X} \mathbf{\beta} + \epsilon \qquad \epsilon \sim \mathcal{N}(0, \sigma)
$$

We could also frame this in another way that is perhaps more Bayesian and will align better with our code below. We could say that our observed values of $\Delta W$ are generated from a normal distribution with mean $\mathbf{X} \mathbf{\beta}$ and standard deviation $\sigma$. Same emperor, new clothes. $\sigma$ then becomes another parameter we will learn from the data.

$$
\Delta W \sim \mathcal{N}(\mathbf{X} \mathbf{\beta}, \sigma)
$$

# A brief aside on fitting models

But, how the heck does this help us to fit a model? What does fitting even mean in a Bayesian framework?

In a Bayesian framework we are not aiming to learn a single set of "true" parameter values for our $\beta$"s and $\sigma$. That would be so very frequentist, and so instead we treat parameters as being probabilistic. It only makes sense to talk about how probable certain values are for the parameters and the range of values that are plausible in our model. Maybe there"s a true value to be found. Theoretically if we had the perfect model and infinite data... yeah no, that"s never going happen modelling an environment system 🥲. 

There"s no reason to get more into frequentist vs Bayesian approaches, I only bring it up to highlight the difference as most people default to frequentist approaches. Both are great, both are useful, both can quantify uncertainty even. But this is a post about Bayes and that"ll be that.

We call the description of probable values for our parameters the posterior distribution. To get to out posterior distribution we need three things:

1. a model that includes a specified likelihood function
2. prior distributions for each of our paramaters
3. data

## Likelihood

The likelihood is a measure of how likely the data are given the model and a set of parameter values. The better fit that a set of parameter values gives, the more we should believe that those values are plausible. Notice the little flip we did there, the Bayes rule helps us to navigate from the likelihood of the data given the parameters to the posterior distribtuion for the paramaters given the data (please see further reading at the end of this section for a more comprehensive introduction to Bayes). Luckily for us, our assumptions about the form of the model and the data generating process that we specified above give rise to a likelihood. By using a Probabilistic Programming Language (PPL) like NumPyro to specify our model, we get it without breaking a sweat.

Let"s have a quick look at an example below. I am going to generate some synthetic data below to relate `x` to `y` with a simple linear relationship plus guassian noise.

```{python}
#| include: false
 
import numpy as np
import numpyro
import numpyro.distributions as dist
import matplotlib.pyplot as plt
import seaborn as sns
np.random.seed(0)
```

```{python}
# make dummy data y = 0.1 x + 0.2 + N(0, 0.1)
x = np.random.randn(100)
y = 0.1 * x + 0.2 + 0.1 * np.random.randn(100)
```

We can specify our linear regression model using a PPL (I"m just teasing this here sorry, we will learn how to do this below). Then, given some propsed values for the parameters, the model will tell us the log-likelihood for these values given the model and the data. Sweet. You can see below that linear fits closer to our $0.1 * x + 0.2$ form data have higher log-likelihoods as we would expect, they fit the data better.

```{python}
#| include: false

# build a simple model of this in NumPyro
def model(x, y=None):
    # Define priors
    beta_0 = numpyro.sample("beta_0", dist.Normal(0, 1))
    beta_1 = numpyro.sample("beta_1", dist.Normal(0, 1))
    sigma = numpyro.sample("sigma", dist.Exponential(1))
    # Define the model
    mu = beta_0 + beta_1 * x
    # store the model prediction before we account for the error
    numpyro.deterministic("mu", mu)
    # and then finally sample so we can compare to our observations
    numpyro.sample("y_out", dist.Normal(mu, sigma), obs=y)

# generate 10 possible solutions
proposed_values = {
    "beta_0" : np.array([0.2, 0.1, 0.05, 0.19, 0.21, 0.4, 0.45, 0.25, 0.05, 0.2]),
    "beta_1" : np.array([0.1, 0.2, 0.05, 0.11, 0.09, 0, -0.1, 0.15, 0.2, 0.3]),
    "sigma" : np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
}
```

```{python}
#| include: false

loglik = numpyro.infer.util.log_likelihood(
    model = model, 
    posterior_samples = proposed_values,
    x = x, 
    y = y
)
```

::: {.callout-note}
For the graph below: the higher the likelihood the better (hence why you may have heard of maximum likelihood estimation).
:::

```{python}
#| echo: false

# plot the predicted line of each of the proposed solutions coloured by the log likelihood and the data
fig, ax = plt.subplots(figsize=(5,3.5))
ax.scatter(x, y, color="black", s=10)

# Get the min and max log likelihood values
vmin = loglik["y_out"].mean(axis=1).min()
vmax = loglik["y_out"].mean(axis=1).max()

for i in range(len(loglik["y_out"])):
    # and weight the width of the line by the log likelihood
    ax.plot(x, x * proposed_values["beta_1"][i] + proposed_values["beta_0"][i], color=plt.cm.viridis((loglik["y_out"][i].mean() - vmin) / (vmax - vmin)), alpha=0.75, lw=2)

ax.set_xlabel("x", fontsize=14)
ax.set_ylabel("y", fontsize=14)
ax.grid(True, which="both", linestyle="--", linewidth=0.5, color="lightgrey")

sm = plt.cm.ScalarMappable(cmap="viridis", norm=plt.Normalize(vmin=vmin, vmax=vmax))
sm.set_array([])
fig.colorbar(sm, ax=ax, label="log-likelihood")
plt.show()
```

## Relax and let the machine do its job

We are going to talk more about prior distrbiutions below, but for now all we need to know is that the model needs us to work with it. Infinity is an infinitely large space, and without some guidance we are sending our model out in the big wide world all alone. We use prior knowledge to define some reasonable space in which we expect to find plausible values for our parameters. The aim is to give the model a nice wide field to explore but not have it wander off a cliff. In environmental science its fairly easy to define what"s silly. For example, can a shoreline erode 100,000 m during a storm event? If it could we"d be in strife so we can pretty safely give very little probability to parameter values that produce such outlandish numbers. We use prior distributions to impart this belief on the model leaving it then explore with this information, the data, and the likelihood function.

Our machine for obtaining samples to build a picture of the posterior distribution for our parameters is going to be Markov Chain Monte Carlo (MCMC). For a great introduction to MCMC see [this lecture](https://youtu.be/rZk2FqX2XnY?si=UpGAmJ3LEGXtGSz-&t=610) from Richard McElreath"s great Statistical Rethinking course.

For our purposes and for the rest of this blog we are going to treat the process of obtaining the posterior distribution as a magic. Please go and read more from many better sources than I to build an understanding of MCMC. But for us, we just specify the data generating process with our model. The NumPyro sampling fairy then floats down from the clouds, pries open the chassis of my laptop and greedily feeds on the power being fed to my CPU for a period of seconds to minutes. The gorged fairy, now satiated, waves its magic wand and voilà! Samples from the posterior distribution are delivered into memory, rendered into figures, and subsequently delivered express to our eyeballs.

::: {.callout-tip}
## Tip
The blood of the CPU will keep you alive, even if you"re an inch from death, but at a terrible price...
:::

- we need to fit the params ($\mu$ or the betas that make up $\mu$, and $\sigma$) based on how well they explain the data
"What this means, recall, is that the machine considers every possible combination of the  parameter values. With a linear model, some of the parameters now stand for the strength  of association between the mean of the outcome, µ, and the value of some other variable.  For each combination of values, the machine computes the posterior probability, which is  a measure of relative plausibility, given the model and data. So the posterior distribution  ranks the infinite possible combinations of parameter values by their logical plausibility."


now what we maybe have a little bit of intuition about how we get the elusive posterior distribution, let"s turn back to our original example. We"re going to look at how create the ingredients for sampling using the numpyro package. for numpyro to work its magic, we need to supply it with a function that:

- takes in the data (covariates (X) and target, y)
- describes the parameters in our model and specifies prior distributions for each
- specifies the model that, given the data and the parameters, estimates the target (y)

really part 2 is the only new part from what the authors did in their paper. so we start by defining a function `linear_model()` that takes in x and y. you"ll notice that y is optional. Our model is always generative and will produce estimates of the target when before fitting (thanks priors). we will use this as a handy check on our priors later in the piece. 

the next bit I want to focus on is the model itself, it specifies the same equation we saw above with all our beta terms and the various columns of the data. this sounds feel familiar and is the engine of our model, defining how we expect the input to be related to the output ( in this case via a simple additive linear model). 

the next part is specifying the form of the residuals or the distribution of observations given the model as we saw above. this defines our likelihood(XX check Vs the stan definition which is different I think as it includes the prior) as discussed above. 

the final part describes our priors. let"s set them all as n(0,1). 

somethings going on here, we have multiple regions of the parameter space showing up as important and our model won"t converge. I"m afraid dear reader I"ve lead you deliberately 
 down the wrong path to make a point. recall above that we are modelling data from 3 locations, each with different characteristics and likely different relationships between covariates and targets. it"s an easy fix to be sure, but I think it shows nicely how transparent a Bayesian model is. because you"re taking many samples from the posterior and visualising the spread, you can identify in a nice way the quirks of your model and data. yes we have taken a bit more time to set it up but at least for me I find the modelling process overall a lot more transparent and this gives me trust. not so much of a problem here where we are fitting a linear model but with more complex models very useful and the next blog post will show an example of this. 


so let"s do this model justice. there are three sites and as the authors identified, there are quite different processes going on at these sites given the level at which we are describing the data. as an aside I like thinking about things in this way, there is no difference in the fundamental physics at each site. quarks are quarks no matter where they are found on the beach. and perhaps physics based models might resolve the process down to the level that it can represent the different observed behaviours given just the profile information and hydrodynamic conditions (though I"ve been skeptical of this in the past based on the models I"ve worked with). but definitely at the coarse grain level we are describing the processes at with our simple linear model, we expect the parameter fits to be different at each site due to the different responses we observe to the same offshore storm.

so we will give each site it"s own parameters which can be fit by the model. luckily we made a nice long table before and that will make it very easy for us to feed in the site parameter another category. if we specify to the model it needs three parameters for each beta, we can use the correct one of those betas for each data point depending on its site. so it model function now becomes. 

you can see below we use plate to generate one parameter per site. and we selected which parameter applies to which data point depending on the site it belongs to given by the site id in cat. (xx just name it site id). 

let"s check our fit. nice, we have a very nicely converging model with parameters per site. 
let"s have a look at these values. I like this information a lot, it points to some of the more interesting points. we are very unsure about whether the direction variable should be in there or not. the bounds are wide and encompass zero. infact if we fit a model without it we do almost as well. again I"m not saying this is not commonplace in frequentist approaches too, but I find this way very clear.  

but as the song goes - when it"s comes to b-b-b-bayesy, you ain"t seen nothing yet. the real power of Bayes comes from the ability of this workflow to scale to much much more complex models. the extra degree of freedom given by the prior, allows us to fully describe our beliefs about the data in a way that we only glimpsed in this example.
